\subsection{Information Flow}

\subsubsection{Overview and Rationale}

Information Flow-based security focuses on confidentiality, as opposed to its integrity or availability. This is in contrast to Access Control mechanisms, which take an approach based on limiting which operations a user or program may perform --- primarily controlling the integrity of a system. Access Control allows the program to assert certain guarantees around what a process can \textit{do}, but it does not directly represent the problem at the heart of confidentiality: how to control when and how confidential data moves through a system.

Information Flow mechanisms, on the other hand, specifically model how data moves through a system, and can therefore provide robust guarantees about that movement. It may be used to address integrity by modelling the flow of potentially `tainted' input through a system. Unlike Access Control, however, Information Flow mechanisms can directly address confidentiality, though there is a relationship between integrity and confidentiality --- they can be seen as duals, where confidentiality concerns the `leakage' of data from the private system to a public output, and integrity concerns the flow of `contaminated' public data to the private system \cite{biba1977integrity} \cite{clarkson2010confintegrity}.

Because Access Control mechanisms (such as Java's stack inspection-based SecurityManager, or Access Control List solutions) focus on modelling access to a system, they can ensure data from an unprivileged (potentially tainted) source cannot reach secure components, thereby providing integrity, but they cannot prevent a user, program or call stack (in the case of SecurityManager) which \textit{has} privileged access from disseminating confidential data in an unauthorised way.

The purpose of Information Flow security is to model the flow of information through a program and prevent data from moving from a high confidentiality state to a low confidentiality state. These are usually modelled by applying Bell and La Padula's Lattice Model \cite{bell1976lattice} of `Mandatory Access Control' to the state of a program and its variables. Either statically (at compile-time) or dynamically (at run-time) the Lattice Model sensitivity labels applied to data are used to prevent execution paths in which high confidentiality data would flow to a low confidentiality state.

\pagebreak

\subsubsection{Lattice Model}

The Lattice Model developed by Bell and La Padula for the US Department of Defense \cite{bell1976lattice} defines a system of `mandatory access controls' which explicitly specify what flows of information are allowed and disallowed. This may be used in conjunction with more traditional discretionary Access Control-based methods, such as Access Control Lists \cite{sandhu1993lattice}, where a user must have access both via the discretionary policy and the mandatory policy.

The Lattice Model defines `security labels' (or \textit{classifications}) on data objects, as well as on subjects (which may be users or programs), where the label is the \textit{clearance} they are operating under \cite{denning1976lattice}.

Formally, the Bell - LaPadula model defines, given a set of security classes $ S $, and a `dominates' relation ($ \ge $) \cite{sandhu1993lattice} which defines a partial ordering on the labels, such that data of security class $ B $ can only flow to any class $ A $ such that $ A \ge B $. This may equivalently be defined, as by Denning \cite{denning1976lattice} as a `flow' relation $ \rightarrow $ which operates in another direction, such that data of class $ B $ may only flow to any class $ A $ such that $ B \rightarrow A $.

This model is applied to the access control problem concerning subjects (users/programs) and objects (data) via two properties \cite{bell1973lattice}:

\begin{description}
	\item[Simple Security Property -- `No Read Up'] A subject with clearance $ C_s $ may only access a data object with classification $ C_o $ if $ C_s \ge C_o $ ($ C_o \rightarrow C_s $)
	\item[* Property -- `No Write Down'] A subject with clearance $ C_s $ may only modify a data object with classification $ C_o $ if $ C_o \ge C_s $ ($ C_s \rightarrow C_o $)
\end{description}

Because of the Simple Security Property, a user with low clearance cannot access data at a high clearance, and because of the * Property, a user with high clearance cannot modify low clearance data, preventing them from `declassifying' high clearance information by gaining simply, for example, copying sensitive information into a file marked at a low classification.

Together, these rules enforce control over information flows: information may only flow from low to high classification, never from high to low. This requires a `high water mark' \cite{jones1975highwatermark} policy, wherein during a program's execution, its security level is monotonically non-decreasing: the clearance it is operating at may rise, but never decrease during execution. For the purposes of Information Flow security, this policy is usually not applied to the linear execution of a program --- it is sufficient to show that no possible execution paths release data to a lower confidentiality level, as both static and dynamic applications of language-based Information Flow security do \cite{sabelfeld2003if}.

\cite{denning1976lattice}

\cite{sandhu1994access}

\subsubsection{Semantics-Based Security}

The purpose of Information Flow security is to show `non-interference' of the program; that is, to show that if two program executions begin with the same state in terms of their low confidentiality values (i.e. the `low view' of the inputs are the same), then the behaviour of the program should be indistinguishable to an attacker \cite{sabelfeld2003if}. This can be formalised in the assertion that a program does not interfere if there is no `strong dependency' \cite{cohen1977declassification} between high and low confidentiality variables.

\subsubsection{Implicit Flows}

Since language-based Information Flow security examines all possible execution paths of a program, it must consider not only what high classification information is revealed by the execution path that occurs, but also by the paths that \textit{don't} occur --- that is, information which is revealed through `implicit' information flows \cite{sabelfeld2003if}.

Denning \& Denning \cite{denning1977if} define an explicit flow $ x \rightarrow y$ as a flow where the operations performed are independent of the value of $ x $ --- such as an assignment statement $ y := x $. They then define an implicit flow $ x \rightarrow y$ as an arbitrary flow from some $ z \rightarrow y $, but where the execution depends upon the value of $ x $, such as where execution branches based on $ x $, as in the following example: $ y := 1; \keyword{if} x = 0 \keyword{then} y := 0 \keyword{else} \text{skip}\;$ \cite{denning1977if}.

In the example above, in the case where the \textbf{then} branch executes, an attacker has gained knowledge of the value of $ x $ even though it was not explicitly modified -- because after the execution occurs, $ y = 0 $, and therefore $ x = 0 $. However, even if the \textbf{else} branch occurs, the attacker still gains information; since $ y = 1 $, the \textbf{then} branch wasn't taken and therefore $ x \ne 0 $.

In order to be considered `secure' from an Information Flow perspective, the program must not reveal information about the state of $ x $ under \textit{any} execution path, whether through implicit or explicit flows.

\subsubsection{Covert Channels}

Consider a program which -- by the definition given in the Implicit Flows section -- is secure. Specifically, consider the following: 

\begin{algorithmic}
	\State $ y := 1 $
	\If {$ x = 0 $}
		\For{$ i := 1 $ \textbf{to} $ 1000000 $}
			\State skip
		\EndFor
	\Else
		\State skip
	\EndIf
\end{algorithmic}

This program does not include any flows, explicit or otherwise, from $ x \rightarrow y $. There is neither any explicit assignment based on $ x $, nor does the control flow dependent on $ x $ produce any observable output in the state of $ y $. However, an adversary able to run this program can still determine information about the value of $ x $ based on the length of time that the program takes to execute. If $ x = 0 $, then the program will run the for loop and take significantly longer to execute than if $ x \ne 0 $ and the else branch is taken.

This is an example of a covert channel attack, and specifically a \textit{timing} channel attack. Sabelfeld and Myers \cite{sabelfeld2003if} consider the impact of a number of kinds of covert channel on information flow security, including timing channels, termination channels (whether a program terminates), probabilistic channels (variance in the probability distributions of program outcomes).

Covert channels present difficulties for Information Flow approaches to security, as even a program which has been formally verified to be secure cannot account for covert channels that may depend upon the environment the program runs in -- for example, covert channel attacks based on the relative CPU time or power draw of an execution path, which may only be possible when the program is run on a particular kind of CPU or system. A program can hence only be said to be `secure' in an Information Flow sense \textit{with respect to a particular environment} \cite{sabelfeld2003if}. This idea is examined by Lampson \cite{lampson1973covertchannels} in considering the `confinement' of programs in order to prevent information flow through unintended channels.	

\subsubsection{Selective Declassification}

While Information Flow models typically attempt to show non-interference --- that low confidentiality data is not affected by high confidentiality data --- in practical applications some flow from high classification to low classification is expected, and indeed necessary. For instance, the output of a program which checks a (confidential) password by necessity depends upon the password's value \cite{sabelfeld2003if}, creating a `declassifying' information flow.

In addition, models of non-interference are conservative: while they may be able to reject any insecure (interfering) program, they also reject some programs that do not interfere. This is not merely a practical limitation of current models: it has been shown that providing a necessary and sufficient condition for the information flow security of a given program is undecidable \cite{landi1992undecidability}, as the halting problem can be reduced to it \cite{denning1977if}.

To alleviate both of these problems, several different mechanisms for controlled `selective declassification' have been proposed, including from Myers and Liskov \cite{myers1997if}, Volpano and Smith \cite{volpano2000declassification}, Mantel and Sands \cite{mantel2004controlled} and others.

Of these, many rely on a principle of `intransitive non-interference'. That is, where information flowing from low to high confidentiality is generally transitive (i.e. $ A \rightarrow B \wedge B \rightarrow C \implies A \rightarrow C $), some information flowing from high to low is allowable, but it is intransitive --- it must pass through particular trusted components. Hence, it may be allowable for information to flow from $ A $ to $ C $, but it \textit{must} pass through classification $ B $ \cite{roscoe1999intransitive}.

Myers and Liskov \cite{myers1997if} propose the implementation of an explicit `declassify' operator which allows for a variable to be declassified, provided the calling code has clearance (this principle is used by the Java Information Flow (JIF) language). This approach is not perfect; it violates the * Property of the Lattice Model, but it requires that the program author explicitly denote when they perform the violation. In addition, it does not provide a way to model the effects of the declassification within the information flow analysis  \cite{zdancewic2004challenges}.

Volpano and Smith \cite{volpano2000declassification} propose a more stringent mechanism for declassification: one which requires that secret information cannot be leaked in polynomial time. Under this model, the password checker example above would be allowable (there is no way for the program to leak the actual password to an attacker except via brute force). While this mechanism provides harder guarantees, it is also more restrictive \cite{zdancewic2004challenges} and hence less practical for writing complex programs.

\subsubsection{Static Information Flow Checking}

Applying Information Flow-based security to real programs requires execution paths of the program to be analysed in order to determine whether any flow from high to low confidentiality occurs. Most implementations of Information Flow security perform some form of static, compile-time analysis in order to accomplish analysis of all possible execution paths while minimising the run-time overhead of the security.

The work of Denning and Denning \cite{denning1977if} forms the basis of many approaches to static Information Flow checking, building a set of `certification semantics' based on the lattice model which apply to particular syntax, in order to certify programs as being secure.

More recent research and most concrete implementations build their static analysis by using \textit{security type systems} \cite{sabelfeld2003if}, which make use of the same static checking and background theory commonly used in statically typed language to enforce Information Flow rules.

Languages with security typing systems associate a classification label (as per the lattice model) with each variable and expression. Volpano and Smith's model \cite{volpano1996sectype} defines a model of 'secure flow types' with an associated set of static checking rules which allow for the security type of any valid (typable) expression to be evaluated. Expressions which contravene a non-interfering information flow policy are not typable under this system. The typing system is compositional: secure expressions within the program are built up and combined to show that the overall program is secure.

An expression or code segment's security type depends on its `security context', modelled as a program-counter label at a particular point \cite{sabelfeld2003if}. In, for example, the context of a high branch (i.e. where the condition is dependent on the value of a high confidentiality variable), code which includes assignments to low confidentiality variables is not typable; thereby preventing both explicit and implicit leaks of information.

Static information flow checking --- and security typing particularly --- forms the basis of most confidentiality-focused Information Flow language extensions. Specifically looking at the Java language as a basis, there are two major efforts to provide developers with languages based on security typing principles.

\paragraph{Java Information Flow (JIF)}

The Java Information Flow language or JIF \cite{myers2006jif}, originally known as JFlow \cite{myers1999jif}, is an extension of the Java language developed at Cornell university which adds security typing in order to allow for static enforcement of both integrity and confidentiality via information flow and access control policies.

JIF represents its security classifications in terms of `principals' (which may represent users, groups of users or other programs) \cite{pullicino2014jif}. The standard lattice model `dominates' relation is defined on these principals in terms of other principals being able to `act' for them (i.e. if Bob allows Alice to `act' for him, then Alice's clearance dominates Bob's and information may flow from Bob to Alice). All variables have a security label associated with them, consisting of both a confidentiality and integrity policy defining how that data is allowed to flow. Methods may also have a principal attached to them as an `authority', indicating that the method acts for that principal.

JIF also provides an explicit `declassify' operator which allows for the selective declassification of information.

\paragraph{Paragon}

Paragon is described in the paper which introduced it \cite{broberg2013paragon} as a `Third-Generation' Information Flow language.

Paragon's design considers earlier languages like JIF and concludes that the necessity for a `declassify' operator essentially limits the security policies that JIF (and languages like it, such as FlowCaml \cite{simonet2003flow}) can represent --- they are locked into the idea of viewing flow policies as ``Good flows plus exceptional cases [requiring selective declassification]" \cite{broberg2013paragon}.

It uses the theory of `Flow Locks' \cite{broberg2010paralocks}: boolean conditions used to specify the conditions required for particular data to flow to a given actor. These conditions are policy independent: the policy specifies when the lock condition becomes true and the lock is `opened' allowing that actor to gain the information, as well as when the lock condition becomes false and the lock is `closed', preventing that information from flowing. Information flows, then, are dependent both on policy and the state of the flow locks. Information may always flow to a more restrictive policy (i.e. one which dominates the existing policy under the lattice model), but declassification is possible through manipulation of the lock state.

The syntax of Paragon models entities in Information Flow as `actors' \cite{broberg2013paragon}, which are similar to principals in JIF, but are modelled as object references rather than as specialised language constructs. Policies are then defined stating which actors information in a given variable is allowed to flow to, as well as a body constraining the policy to flow lock predicates.

Locks may then be used to allow for selective declassification of data, however Paragon will act conservatively in its evaluation of lock states in order to guard against leaks in implicit flows \cite{broberg2013paragon}, since the state of the lock at the time the implicit flow is triggered (e.g. by a high conditional) may be different to the state at the time the information is actually revealed.

Paragon's analysis of lock states is purely static, though the lock expressions may be used as first class values at run-time, though the authors of Paragon express interest in the possibility of extending the idea of flow locks to dynamic Information Control enforcement mechanisms \cite{broberg2013paragon}.

\subsubsection{Issues with the Static Checking Approach}

While static checking has advantages in performance, and in the ease of checking for implicit flows, it does have downsides. One major issue is the inability of static checking systems to deal with labels which may change before or during execution (such as file permissions) \cite{sabelfeld2003if}.

One solution to this issue is to extend static checking policies by allowing labels to be used as first class values, meaning variables may be annotated with a security type which is only known at run-time \cite{myers1997if}. This can be performed using dependent types \cite{xi1999dependenttypes}, and is implemented by the JIF compiler \cite{myers1999jif}. This kind of modelling is complex, however, and still cannot account for the level of dynamism in permissions any large program will have to face --- consider, for example, a web browser \cite{venkatakrishnan2006runtime}. A browser by definition will have to interact with a multitude of websites over a long period of time, and the confidentiality state of all information the browser will ever process simply cannot be modelled at compile-time (for the specific case of the web browser, a system based on Secure Multi-Execution \cite{devriese2010sme}, discussed further below, has been implemented to create an Information Flow secure browser called FlowFox \cite{degroef2012flowfox}).

Venkatakrishnan et al. \cite{venkatakrishnan2006runtime} consider two other issues facing static checking approaches. Firstly, since statically analysing implicit flows is undecidable \cite{landi1992undecidability}, by necessity static analysis makes use of conservative approximations to ensure that the analysis terminates.

Secondly, static analysis cannot support programs that may \textit{occasionally} have the potential to leak information \cite{venkatakrishnan2006runtime} --- for example, a program which sends a crash report has the potential to leak sensitive information in that report, but a static analysis cannot dynamically choose whether to allow the report or not; it instead must disallow \textit{all} reports, even if most would not cause information leaks.

\subsubsection{Run-Time Information Flow Checking}

While concerns about performance and the ability to analyse all execution paths have resulted in most Information Flow security research and implementation being focused on static analysis, attempts at building run-time enforcement systems have also been made, in order to find systems which can correct some of the flaws of static checking methodologies.

Allowing dynamic or variable permissions is simple in a run-time enforced system. Similarly, code which may only produce leaks under certain circumstances, such as a crash report, can be accounted for by performing a run-time check before that action is executed. In addition, a run-time enforced system does not need to approximate in order to make the analysis decidable, as only the flow actually being executed is analysed \cite{venkatakrishnan2006runtime}.

The primary drawback to using run time enforcement is the difficulty in detecting implicit flows \cite{sabelfeld2003if} --- since only the run-time execution path is analysed, implicit flows that depend on the different branching possibilities cannot be picked up, where a static analysis would be able to infer their existence by analysing the program's structure \cite{venkatakrishnan2006runtime}. It is, hence impossible to dynamically show non-interference using an `execution monitoring' enforcement system alone \cite{schneider2000executionmonitoring}.

Nonetheless, run-time enforced information flow tracking systems have been produced, both for tracking confidentiality and integrity (as in systems like GIFT \cite{lam2006dynamicintegrity}), and alterations and hybrid strategies have been used to mitigate their shortcomings.

Suh et al. \cite{suh2004dynamicintegrity} design a `dynamic information flow tracking system' designed to prevent and mitigate software attacks. Primarily, they focus on integrity rather than confidentiality, considering the potential sources of tainted data in order to build policies about what input and output flows are acceptable. Their mechanism does not, however, attempt to show non-interference and hence cannot provide true guarantees around confidentiality.

Austin and Flanagan (2010) \cite{austin2010runtime} propose the use of a `permissive upgrade' strategy to mitigate the confidentiality damage of leaks caused by implicit flows. Their system allows for the `partial leak' of information via implicit flows (i.e. those which reveal information about the confidential data, but do not fully expose it), by adding a special `partially leaked' label at runtime to any information which may have been partially leaked by an implicit flow, and causing sensitive operations (such as assignment and conditional expressions) to fail to terminate if applied to partially leaked data. Hence, non-interference is preserved, but is termination-insensitive (i.e. a program may be forced to simply not terminate rather than leak information). This does itself cause a minor termination channel leak --- if the program doesn't terminate, then the attacker does learn some information about the state of the partially leaked variable. The paper hence designs their system to minimise the number of situations in which non-termination occurs, balancing the trade-off between termination sensitivity and non-interference.

Venkatakrishnan et al. \cite{venkatakrishnan2006runtime} propose a hybrid technique that combines some static analysis techniques with a primarily run-time enforced system. They make a distinction in implicit flows between `positive control dependencies' (e.g. flows caused a high conditional causing different assignments to a low variable to occur) and `negative control dependencies' (e.g. flows where some values of a high conditional may cause an assignment to a low variable to be skipped); dynamic run-time policies are used to prevent positive control dependencies, whereas negative control dependencies (which cannot be caught by conventional run-time analysis techniques) are caught using static analysis.

Austin and Flanagan (2009) \cite{austin2009dynamicif} use a purely dynamic approach to track information flow in JavaScript (a dynamically typed language), by making use of `label locality' --- i.e. the fact that most items in a given data structure will have the same label. They use `Sparse Labelling', inferring labels from context wherever possible (as opposed to the standard `Universal Labelling' approach). They show experimentally that the sparse labelling approach provides a significant performance improvement over a dynamic Universal Labelling approach, thereby mitigating the impact of the overhead issue typical of run-time enforced systems.

\subsubsection{Secure Multi-Execution}

A more radical alternative to existing static and dynamic enforcement mechanisms is the technique of `Secure Multi-Execution. As defined by Devriese and Piessens \cite{devriese2010sme}, Secure Multi-Execution aims to circumvent the limitations of both static and dynamic approaches (specifically, the conservative approximation required by static monitoring and the inability to dynamically determine non-interference with execution monitoring \cite{schneider2000executionmonitoring}).

With Secure Multi-Execution, a program is executed multiple times (one per security classification), with inputs replaced by defaults except during execution at the security clearance of that input channel or higher, and outputs only produced during the execution at the appropriate security clearance for that channel. Devriese and Piessens \cite{devriese2010sme} showed that this mechanism can prove non-interference for a program, and argued that though it requires overhead in terms of CPU and memory usage, it is practical within the setting of a web browser's JavaScript engine. The FlowFox web browser \cite{degroef2012flowfox} was built upon this design.
	