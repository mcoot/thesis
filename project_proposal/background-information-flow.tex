\subsection{Information Flow}

\subsubsection{Overview and Rationale}

Information Flow-based security focuses on confidentiality, as opposed to its integrity or availability. This is in contrast to Access Control mechanisms, which take an approach based on limiting which operations a user or program may perform --- controlling confidentiality but also the integrity of a system. Access Control allows the program to assert certain guarantees around what a process can \textit{do}, but it does not directly represent the problem at the heart of confidentiality: how to control when and how confidential data moves through a system.

Information Flow mechanisms, on the other hand, specifically model how confidential data moves through a system, and can therefore provide robust guarantees about that movement. It does not specifically address integrity at all, though there is a relationship between integrity and confidentiality --- they can be seen as duals, where confidentiality concerns the `leakage' of data from the private system to a public output, and integrity concerns the flow of `contaminated' public data to the private system \cite{biba1977integrity} \cite{clarkson2010confintegrity}.

Because Access Control mechanisms (such as Java's stack inspection-based SecurityManager, or Access Control List solutions) focus on modelling access to a system, they can ensure `tainted' data does not reach secure elements, thereby providing integrity, but they cannot prevent a user, program or call stack (in the case of SecurityManager) which \textit{has} privileged access from disseminating confidential data in an unauthorised way (REFERENCE).

The purpose of Information Flow security is to model the flow of information through a program and prevent data from moving from a high confidentiality state to a low confidentiality state. These are usually modelled by applying Bell and La Padula's Lattice Model \cite{bell1976lattice} of `Mandatory Access Control' to the state of a program and its variables. Either statically (at compile-time) or dynamically (at run-time) the Lattice Model sensitivity labels applied to data are used to prevent execution paths in which high confidentiality data would flow to a low confidentiality state.

\pagebreak

\subsubsection{Principles}

	\paragraph{Lattice Model}
	
	The Lattice Model developed by Bell and La Padula for the US Department of Defense \cite{bell1976lattice} defines a system of `mandatory access controls' which explicitly specify what flows of information are allowed and disallowed. This may be used in conjunction with more traditional discretionary Access Control-based methods, such as Access Control Lists \cite{sandhu1993lattice}, where a user must have access both via the discretionary policy and the mandatory policy.
	
	The Lattice Model defines `security labels' (or \textit{classifications}) on data objects, as well as on subjects (which may be users or programs), where the label is the \textit{clearance} they are operating under \cite{denning1976lattice}.
	
	Formally, the Bell - LaPadula model defines, given a set of security classes $ S $, and a `dominates' relation ($ \ge $) \cite{sandhu1993lattice} which defines a partial ordering on the labels, such that data of security class $ B $ can only flow to any class $ A $ such that $ A \ge B $. This may equivalently be defined, as by Denning \cite{denning1976lattice} as a `flow' relation $ \rightarrow $ which operates in another direction, such that data of class $ B $ may only flow to any class $ A $ such that $ B \rightarrow A $.
	
	This model is applied to the access control problem concerning subjects (users/programs) and objects (data) via two properties \cite{bell1973lattice}:
	
	\begin{description}
		\item[Simple Security Property -- `No Read Up'] A subject with clearance $ C_s $ may only access a data object with classification $ C_o $ if $ C_s \ge C_o $ ($ C_o \rightarrow C_s $)
		\item[* Property -- `No Write Down'] A subject with clearance $ C_s $ may only modify a data object with classification $ C_o $ if $ C_o \ge C_s $ ($ C_s \rightarrow C_o $)
	\end{description}
	
	Because of the Simple Security Property, a user with low clearance cannot access data at a high clearance, and because of the * Property, a user with high clearance cannot modify low clearance data, preventing them from `declassifying' high clearance information by gaining simply, for example, copying sensitive information into a file marked at a low classification.
	
	Together, these rules enforce control over information flows: information may only flow from low to high classification, never from high to low. This requires a `high water mark' \cite{jones1975highwatermark} policy, wherein during a program's execution, its security level is monotonically non-decreasing: the clearance it is operating at may rise, but never decrease during execution. For the purposes of Information Flow security, this policy is usually not applied to the linear execution of a program --- it is sufficient to show that no possible execution paths release data to a lower confidentiality level, as both static and dynamic applications of language-based Information Flow security do \cite{sabelfeld2003if}.
	
	\cite{denning1976lattice}
	
	\cite{sandhu1994access}
	
	\paragraph{Semantics-Based Security}
	
	The purpose of Information Flow security is to show `non-interference' of the program; that is, to show that if two program executions begin with the same state in terms of their low confidentiality values (i.e. the `low view' of the inputs are the same), then the behaviour of the program should be indistinguishable to an attacker \cite{sabelfeld2003if}. This can be formalised in the assertion that a program does not interfere if there is no `strong dependency' \cite{cohen1977declassification} between high and low confidentiality variables.
	
	\paragraph{Implicit Flows}
	
	Since language-based Information Flow security examines all possible execution paths of a program, it must consider not only what high classification information is revealed by the execution path that occurs, but also by the paths that \textit{don't} occur --- that is, information which is revealed through `implicit' information flows \cite{sabelfeld2003if}.
	
	Denning \& Denning \cite{denning1977if} define an explicit flow $ x \rightarrow y$ as a flow where the operations performed are independent of the value of $ x $ --- such as an assignment statement $ y := x $. They then define an implicit flow $ x \rightarrow y$ as an arbitrary flow from some $ z \rightarrow y $, but where the execution depends upon the value of $ x $, such as where execution branches based on $ x $, as in the following example: $ y := 1; \keyword{if} x = 0 \keyword{then} y := 0 \keyword{else} \text{skip}\;$ \cite{denning1977if}.
	
	In the example above, in the case where the \textbf{then} branch executes, an attacker has gained knowledge of the value of $ x $ even though it was not explicitly modified -- because after the execution occurs, $ y = 0 $, and therefore $ x = 0 $. However, even if the \textbf{else} branch occurs, the attacker still gains information; since $ y = 1 $, the \textbf{then} branch wasn't taken and therefore $ x \ne 0 $.
	
	In order to be considered `secure' from an Information Flow perspective, the program must not reveal information about the state of $ x $ under \textit{any} execution path, whether through implicit or explicit flows.
	
	\paragraph{Covert Channels}
	
	Consider a program which -- by the definition given in the Implicit Flows section -- is secure. Specifically, consider the following: 
	
	\begin{algorithmic}
		\State $ y := 1 $
		\If {$ x = 0 $}
			\For{$ i := 1 $ \textbf{to} $ 1000000 $}
				\State skip
			\EndFor
		\Else
			\State skip
		\EndIf
	\end{algorithmic}
	
	This program does not include any flows, explicit or otherwise, from $ x \rightarrow y $. There is neither any explicit assignment based on $ x $, nor does the control flow dependent on $ x $ produce any observable output in the state of $ y $. However, an adversary able to run this program can still determine information about the value of $ x $ based on the length of time that the program takes to execute. If $ x = 0 $, then the program will run the for loop and take significantly longer to execute than if $ x \ne 0 $ and the else branch is taken.
	
	This is an example of a covert channel attack, and specifically a \textit{timing} channel attack. Sabelfeld and Myers \cite{sabelfeld2003if} consider the impact of a number of kinds of covert channel on information flow security, including timing channels, termination channels (whether a program terminates), probabilistic channels (variance in the probability distributions of program outcomes).
	
	Covert channels present difficulties for Information Flow approaches to security, as even a program which has been formally verified to be secure cannot account for covert channels that may depend upon the environment the program runs in -- for example, covert channel attacks based on the relative CPU time or power draw of an execution path, which may only be possible when the program is run on a particular kind of CPU or system. A program can hence only be said to be `secure' in an Information Flow sense \textit{with respect to a particular environment} \cite{sabelfeld2003if}. This idea is examined by Lampson \cite{lampson1973covertchannels} in considering the `confinement' of programs in order to prevent information flow through unintended channels.	
	
	\paragraph{Selective Declassification}
	
	While Information Flow models typically attempt to show non-interference --- that low confidentiality data is not affected by high confidentiality data --- in practical applications some flow from high classification to low classification is expected, and indeed necessary. For instance, the output of a program which checks a (confidential) password by necessity depends upon the password's value \cite{sabelfeld2003if}, creating a `declassifying' information flow.
	
	In addition, models of non-interference are conservative: while they may be able to reject any insecure (interfering) program, they also reject some programs that do not interfere. This is not merely a practical limitation of current models: it has been shown that providing a necessary and sufficient condition for the information flow security of a given program is undecidable, as the halting problem can be reduced to it \cite{denning1977if}.
	
	To alleviate both of these problems, several different mechanisms for controlled `selective declassification' have been proposed, including from Myers and Liskov \cite{myers1997if}, Volpano and Smith \cite{volpano2000declassification}, Mantel and Sands \cite{mantel2004controlled} and others.
	
	Of these, many rely on a principle of `intransitive non-interference'. That is, where information flowing from low to high confidentiality is generally transitive (i.e. $ A \rightarrow B \wedge B \rightarrow C \implies A \rightarrow C $), some information flowing from high to low is allowable, but it is intransitive --- it must pass through particular trusted components. Hence, it may be allowable for information to flow from $ A $ to $ C $, but it \textit{must} pass through classification $ B $ \cite{roscoe1999intransitive}.
	
	Myers and Liskov \cite{myers1997if} propose the implementation of an explicit `declassify' operator which allows for a variable to be declassified, provided the calling code has clearance (this principle is used by the Java Information Flow (JIF) language). This approach is not perfect; it violates the * Property of the Lattice Model, but it requires that the program author explicitly denote when they perform the violation. In addition, it does not provide a way to model the effects of the declassification within the information flow analysis  \cite{zdancewic2004challenges}.
	
	Volpano and Smith \cite{volpano2000declassification} propose a more stringent mechanism for declassification: one which requires that secret information cannot be leaked in polynomial time. Under this model, the password checker example above would be allowable (there is no way for the program to leak the actual password to an attacker except via brute force). While this mechanism provides harder guarantees, it is also more restrictive \cite{zdancewic2004challenges} and hence less practical for writing complex programs.
	
	\paragraph{Static Information Flow Checking}
	
	Applying Information Flow-based security to real programs requires all possible execution paths of the program to be analysed in order to determine whether any flow from high to low confidentiality occurs. Most implementations of Information Flow security perform some form of static, compile-time analysis in order to accomplish this.
	
	The work of Denning and Denning \cite{denning1977if} forms the basis of many approaches to static Information Flow checking, building a set of `certification semantics' based on the lattice model which apply to particular syntax, in order to certify programs as being secure.
	
	More recent research and most concrete implementations build their static analysis by using \textit{security type systems} \cite{sabelfeld2003if}, which make use of the same static checking and background theory commonly used in statically typed language to enforce Information Flow rules.
	
	Languages with security typing systems associate a classification label (as per the lattice model) with each variable and expression. Volpano and Smith's model \cite{volpano1996sectype} defines a model of 'secure flow types' with an associated set of static checking rules which allow for the security type of any valid (typable) expression to be evaluated. Expressions which contravene a non-interfering information flow policy are not typable under this system. The typing system is compositional: secure expressions within the program are built up and combined to show that the overall program is secure.
	
	An expression or code segment's security type depends on its `security context', modelled as a program-counter label at a particular point \cite{sabelfeld2003if}. In, for example, the context of a high branch (i.e. where the condition is dependent on the value of a high confidentiality variable), code which includes assignments to low confidentiality variables is not typable; thereby preventing both explicit and implicit leaks of information.
	
	DEPENDENT TYPES
	
	\paragraph{Run-Time Information Flow Checking}
	
	While the need to evaluate all execution paths and concerns about performance have resulted in most Information Flow security research and implementation being focused on static analysis, attempts at building run-time enforcement systems have also been made. One motivation for this is that statically checked information flow security cannot enforce policies which depend on information only available at run-time, or information which may change before or during execution (such as file permissions) \cite{sabelfeld2003if}.
	
	One solution to this issue is to extend static checking policies by allowing labels to be used as first class values, meaning variables may be annotated with a security type which is only known at run-time \cite{myers1997if}. This can be performed using dependent types \cite{xi1999dependenttypes}, and is implemented by the JIF compiler \cite{myers1999jif}.
	
	True run-time solutions must account for the difficulty of detecting implicit flows in a dynamic context.
	
	\cite{austin2009dynamicif}
	\cite{venkatakrishnan2006runtime}
	
	\cite{austin2010runtime}
	
\subsubsection{Security Model Implementations}

	\paragraph{Java Information Flow (JIF)}
	
	\cite{myers1999jif}
	
	\cite{pullicino2014jif}
	
	\cite{broberg2013paragon}
	
	\paragraph{Paragon}